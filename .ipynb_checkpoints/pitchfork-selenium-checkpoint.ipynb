{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-agent: *\n",
      "Disallow: /wp-admin/\n",
      "Allow: /wp-admin/admin-ajax.php\n",
      "\n",
      "Sitemap: https://jazztimes.com/sitemap_index.xml\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://www.jazztimes.com/robots.txt'\n",
    "response  = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-agent: *\n",
      "Allow: /\n",
      "Disallow: /search/\n",
      "Disallow: /search\n",
      "\n",
      "Sitemap: https://pitchfork.com/sitemap.xml\n",
      "Sitemap: https://pitchfork.com/branded-sitemap.xml\n",
      "Sitemap: https://pitchfork.com/feed/google-latest-news/sitemap-google-news\n",
      "Sitemap: https://pitchfork.com/feed/sitemap\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://www.pitchfork.com/robots.txt'\n",
    "response  = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sitemap: https://www.stereogum.com/sitemap.xml\r\n",
      "Sitemap: https://www.stereogum.com/sitemap_news.xml\r\n",
      "User-agent: *\r\n",
      "Disallow: /wp/wp-admin/\r\n",
      "Disallow: /wp-includes/\r\n",
      "Disallow: /xmlrpc.php\r\n",
      "Disallow: /*?s=\r\n",
      "Disallow: /search/*\r\n",
      "Disallow: /6419/stereogum/*\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://www.stereogum.com/robots.txt'\n",
    "response  = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## allows us to use reg expressions to search fields\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "# user agent\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "options = Options()\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "print(userAgent)\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "\n",
    "#driver = webdriver.Chrome(chrome_options=options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Selenium to scrape album contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-604fb710ee1e>:1: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(chromedriver,chrome_options=options)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(chromedriver,chrome_options=options)\n",
    "\n",
    "# update the page here to start where left off. Page is essentially the count as well\n",
    "pagestart=1\n",
    "pageend=2\n",
    "\n",
    "#_url = \"https://www.discogs.com/sell/list?sort=price%2Cdesc&limit=100&year1=1900&year2=1970&format=Vinyl&price=over40&genre=Jazz&currency=USD&style=Hard+Bop&page=\"\n",
    "# not just bebop\n",
    "_url = \"https://jazztimes.com/reviews/albums/page/\"\n",
    "url= _url + str(pagestart) \n",
    "\n",
    "# starting url\n",
    "#driver.get(\"https://www.discogs.com/sell/list?sort=price%2Cdesc&limit=100&year1=1900&year2=1970&format=Vinyl&price=over40&genre=Jazz&currency=USD&style=Hard+Bop&page=3#more%3Dyear\")\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "with open('recorddata.csv', 'w',newline='') as csvfile:\n",
    "    file = csv.writer(csvfile)\n",
    "    # make headers\n",
    "    file.writerow(['Album','title','contents'])\n",
    "    count = pagestart #3\n",
    "    \n",
    "    while count < pageend: # 7:\n",
    "        \n",
    "        # find all links on album marketplace page and store in list\n",
    "\n",
    "        result_elements = '//a[contains(@href, \"/reviews/albums/\")]'\n",
    "\n",
    "        albums = []\n",
    "\n",
    "        albumdriver = driver.find_elements_by_xpath(result_elements)\n",
    "\n",
    "        for url in albumdriver:\n",
    "            albums.extend([url.get_attribute('href')])\n",
    "        \n",
    "        # get rid of duplicates\n",
    "\n",
    "        albumsclean = [album.split('?', 1)[0] for album in albums]\n",
    "        albumurls = set(albumsclean)\n",
    "\n",
    "        # scrape info from album page\n",
    "\n",
    "        for album in albumurls:\n",
    "            \n",
    "            driver.get(album)\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "#             try:\n",
    "#                 artist_album = driver.find_element_by_xpath(\"//h1[contains(@id, 'profile_title')]\").text.strip()\n",
    "#             except:\n",
    "#                 artist_album = np.nan\n",
    "        \n",
    "            ## cc: label\n",
    "#             try:\n",
    "#                 album = driver.find_element_by_xpath(\"//a[contains(text(), 'bookmark:')]/following-sibling::div\").text.strip()\n",
    "#             except:\n",
    "#                 album = np.nan\n",
    "            try:\n",
    "                contents = soup.find(class_='entry-content').text.strip()\n",
    "            except:\n",
    "                contents = np.nan    \n",
    "            \n",
    "#             try:\n",
    "#                 album = driver.find_element_by_xpath(\"//a[contains(text(), 'bookmark')]\").text.strip()\n",
    "#             except:\n",
    "#                 album = np.nan    \n",
    "            ## cc: country\n",
    "#             try:\n",
    "#                 country = driver.find_element_by_xpath(\"//div[contains(text(), 'Country:')]/following-sibling::div\").text.strip()\n",
    "#             except:\n",
    "#                 country = np.nan    \n",
    "            \n",
    "#             ## cc: format\n",
    "#             try:\n",
    "#                 rformat = driver.find_element_by_xpath(\"//div[contains(text(), 'Format:')]/following-sibling::div\").text.strip()\n",
    "#             except:\n",
    "#                 rformat = np.nan    \n",
    "                \n",
    "#             ## cc: notes\n",
    "#             try:\n",
    "#                 notes = driver.find_element_by_xpath(\"//h3[contains(text(), 'Notes')]/following-sibling::div\").text.strip()\n",
    "#             except:\n",
    "#                 notes = np.nan    \n",
    "                        \n",
    "            \n",
    "#             try:\n",
    "#                 genre = driver.find_element_by_xpath(\"//div[contains(text(), 'Genre:')]/following-sibling::div\").text.strip()\n",
    "#             except:\n",
    "#                 genre = np.nan\n",
    "\n",
    "#             try:\n",
    "#                 release_date = driver.find_element_by_xpath(\"//div[contains(text(), 'Released:')]/following-sibling::div\").text.strip()\n",
    "#             except:\n",
    "#                 release_date = np.nan\n",
    "\n",
    "#             try:\n",
    "#                 style = driver.find_element_by_xpath(\"//div[contains(text(), 'Style:')]/following-sibling::div\").text.strip()\n",
    "#             except:\n",
    "#                 style = np.nan\n",
    "\n",
    "#             try:\n",
    "#                 rate_haves_wants = driver.find_element_by_xpath(\"//a[contains(@class, 'button-blue')]/following-sibling::div\").text.strip()\n",
    "#             except:\n",
    "#                 rate_haves_wants = np.nan\n",
    "\n",
    "#             try:\n",
    "#                 m_condition = driver.find_element_by_xpath(\"//strong[contains(text(), 'Media:')]/following-sibling::span\").text.strip()\n",
    "#             except:\n",
    "#                 m_condition = np.nan\n",
    "    \n",
    "#             try:\n",
    "#                 sleeve = driver.find_element_by_xpath(\"//strong[contains(text(), 'Sleeve:')]\")\n",
    "#                 s_condition = sleeve.find_element_by_xpath('..').text.strip()\n",
    "#             except:\n",
    "#                 s_condition = np.nan\n",
    "\n",
    "#             try:\n",
    "#                 seller_rating = driver.find_element_by_xpath(\"//span[@class='star_rating']/following-sibling::strong\").text.strip()\n",
    "#             except:\n",
    "#                 seller_rating = np.nan\n",
    "\n",
    "#             # recorded at        \n",
    "#             try:\n",
    "#                 recorded_at = driver.find_element_by_xpath(\"//span[contains(text(),'Recorded At')]/following-sibling::a\").text.strip()\n",
    "#             except:\n",
    "#                 recorded_at = np.nan    \n",
    "                \n",
    "#             # pressed at\n",
    "#             try:\n",
    "#                 pressed_by = driver.find_element_by_xpath(\"//span[contains(text(),'Pressed By')]/following-sibling::a\").text.strip()\n",
    "#             except:\n",
    "#                 pressed_by = np.nan    \n",
    "    \n",
    "\n",
    "            try:\n",
    "                title = soup.find(class_='entry-title').text.strip()\n",
    "            except:\n",
    "                title = np.nan\n",
    "\n",
    "            observation = [album,title,contents]\n",
    "\n",
    "            file.writerow(observation)\n",
    "\n",
    "            time.sleep(.5+2*random.random())\n",
    "            \n",
    "        # go to next page in the marketplace\n",
    "        \n",
    "        #nextpage = \"https://www.discogs.com/sell/list?sort=listed%2Casc&currency=USD&limit=25&page=\" + str(count+1) + \"&format=Vinyl\"\n",
    "        #nextpage = \"https://www.discogs.com/sell/list?sort=price%2Cdesc&limit=100&year1=1900&year2=1970&format=Vinyl&price=over40&genre=Jazz&currency=USD&style=Hard+Bop&page=\" + str(count+1) + \"#more%3Dyear\"\n",
    "        nextpage = \"https://jazztimes.com/reviews/albums/page/\" + str(count+1)\n",
    "        driver.get(nextpage)\n",
    "        \n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
